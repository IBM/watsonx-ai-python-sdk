Model
=====

.. autoclass:: ibm_watsonx_ai.foundation_models.Model
   :members:
   :exclude-members:
   :undoc-members:
   :show-inheritance:

Enums
-----

.. autoclass:: metanames.GenTextParamsMetaNames
    :members:

.. autoclass:: metanames.GenTextReturnOptMetaNames
    :members:

.. autoclass:: ibm_watsonx_ai.foundation_models.utils.enums.DecodingMethods
    :members:
    :undoc-members:
    :show-inheritance:

.. class:: TextModels

    Bases: ``Enum``

    This represents a dynamically generated Enum for Foundation Models.

    **Example of getting TextModels**

    .. code-block:: python

        # GET TextModels ENUM
        client.foundation_models.TextModels

        # PRINT dict of Enums
        client.foundation_models.TextModels.show()

    **Example Output:**

    .. code-block::

        {'CODELLAMA_34B_INSTRUCT_HF': 'codellama/codellama-34b-instruct-hf',
        'FLAN_T5_XL': 'google/flan-t5-xl',
        'FLAN_T5_XXL': 'google/flan-t5-xxl',
        'FLAN_UL2': 'google/flan-ul2',
        'GRANITE_13B_CHAT_V2': 'ibm/granite-13b-chat-v2',
        'GRANITE_13B_INSTRUCT_V2': 'ibm/granite-13b-instruct-v2',
        'GRANITE_20B_CODE_INSTRUCT': 'ibm/granite-20b-code-instruct',
        'GRANITE_20B_MULTILINGUAL': 'ibm/granite-20b-multilingual',
        'GRANITE_34B_CODE_INSTRUCT': 'ibm/granite-34b-code-instruct',
        'GRANITE_3B_CODE_INSTRUCT': 'ibm/granite-3b-code-instruct',
        'GRANITE_7B_LAB': 'ibm/granite-7b-lab',
        ...
        'GRANITE_8B_CODE_INSTRUCT': 'ibm/granite-8b-code-instruct',
        'LLAMA_2_13B_CHAT': 'meta-llama/llama-2-13b-chat',
        'LLAMA_2_70B_CHAT': 'meta-llama/llama-2-70b-chat',
        'LLAMA_3_70B_INSTRUCT': 'meta-llama/llama-3-70b-instruct',
        'LLAMA_3_8B_INSTRUCT': 'meta-llama/llama-3-8b-instruct',
        'MERLINITE_7B': 'ibm-mistralai/merlinite-7b',
        'MIXTRAL_8X7B_INSTRUCT_V01': 'mistralai/mixtral-8x7b-instruct-v01',
        'MIXTRAL_8X7B_INSTRUCT_V01_Q': 'ibm-mistralai/mixtral-8x7b-instruct-v01-q',
        'MT0_XXL': 'bigscience/mt0-xxl'}

    **Example of initialising ModelInference with TextModels Enum:**

    .. code-block:: python

        from ibm_watsonx_ai.foundation_models import ModelInference

        model = ModelInference(
            model_id=client.foundation_models.TextModels.GRANITE_13B_INSTRUCT_V2,
            credentials=Credentials(...),
            project_id=project_id,
        )

.. autoclass:: ibm_watsonx_ai.foundation_models.utils.enums.ModelTypes
    :members:
    :show-inheritance:
